{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reaching multiple targets with a manipulator\n",
    "The objective of this exercice is to introduce the implementation of the front-end based on Pinocchio, for polyarticulated systems modeled in position-velocity-torque.\n",
    "\n",
    "We provide a basic example for reaching one point with a manipulator robot. You are then guided to modify this example for sequence of multiple targets. \n",
    "\n",
    "## Front-end and back-end\n",
    "Our optimal control toolbox is composed of 3 main parts: the backend contains the API for solving the problem and accessing the solution and implements several solvers, DDP being the first one; the frontend is first composed of a basic API which we mostly used in the exercice with unicycle and bicopter, and that you can use to implement any fancy problem; finally, a particular implementation of the front end using Pinocchio is mostly written for working with polyarticulated systems such as manipulator robots, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gepetuto.magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Panda reaches a single target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment \n",
    "We will need crocoddyl as in the previous notebook, with the model of the arm of the humanoid robot Talos, a 7-dof arm. It can be found in example robot data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_import\n",
    "import crocoddyl\n",
    "import pinocchio as pin\n",
    "import numpy as np\n",
    "import example_robot_data as robex\n",
    "import matplotlib.pylab as plt\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the Pinocchio model for the Panda arm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_robexload\n",
    "# First, let's load the Pinocchio model for the Panda arm.\n",
    "robot = robex.load('panda')\n",
    "# The 2 last joints are for the fingers, not important in arm motion, freeze them\n",
    "robot.model,[robot.visual_model,robot.collision_model] = \\\n",
    "    pin.buildReducedModel(robot.model,[robot.visual_model,robot.collision_model],[8,9],robot.q0)\n",
    "robot.q0 = robot.q0[:7].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal control problem is defined by a bunch of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_hyperparameters\n",
    "HORIZON_LENGTH = 100\n",
    "TIME_STEP = 1e-2\n",
    "FRAME_TIP = robot.model.getFrameId(\"panda_hand_tcp\")\n",
    "GOAL_POSITION = np.array([.2,0.5,.5])\n",
    "GOAL_PLACEMENT = pin.SE3(pin.utils.rpyToMatrix(-np.pi,0,np.pi/4), GOAL_POSITION)\n",
    "REACH_DIMENSION = \"3d\" # \"6d\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set robot model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp5/generated/arm_example_robot_model\n",
    "# Set robot model\n",
    "robot_model = robot.model\n",
    "robot_model.armature = np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.])*5\n",
    "robot_model.q0 = np.array([3.5,2,2,0,0,0,0])\n",
    "robot_model.x0 = np.concatenate([robot_model.q0, np.zeros(robot_model.nv)])\n",
    "robot_model.gravity *= 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use meshcat for displaying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_viz\n",
    "from utils.meshcat_viewer_wrapper import MeshcatVisualizer\n",
    "viz = MeshcatVisualizer(robot)\n",
    "viz.display(robot.q0)\n",
    "viz.addBox('world/goal',[.1,.1,.1],[0,1,0,1])\n",
    "viz.applyConfiguration('world/goal',GOAL_PLACEMENT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viewer.jupyter_cell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is augmented with a default state $x_0$, and armature is added to the joints to model the gear reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_robot_model\n",
    "# Set robot model\n",
    "robot_model = robot.model\n",
    "robot_model.armature = np.ones(robot.model.nv)*2 # Arbitrary value representing the true armature\n",
    "robot_model.q0 = robot.q0.copy()\n",
    "robot_model.x0 = np.concatenate([robot_model.q0, np.zeros(robot_model.nv)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the action model combining cost and dynamics\n",
    "This starts with specifying the state space, defined by $x=(q,v)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_state\n",
    "state = crocoddyl.StateMultibody(robot_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost is defined as a sum of multiple terms. The classes CostSum are created to store the terms. Let's have one for the running nodes, and one for the terminal node.\n",
    "\n",
    "We need to first define a cost model (i.e. set of cost functions) in order to next define the action model for our optimal control problem.\n",
    "For this particular example, we formulate three running-cost functions: goal-tracking cost, state and control regularization; and a terminal cost: goal cost. First, let's create the common cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_sumofcosts\n",
    "runningCostModel = crocoddyl.CostModelSum(state)\n",
    "terminalCostModel = crocoddyl.CostModelSum(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a cost for reaching the target, either as a 3D objective $\\ell(x) = \\| p(q)-p^*\\|^2$, or as a 6D objective $\\ell(x) = \\| \\log( \\ ^0M_E(q)^{-1} \\ ^0M_* ) \\|^2$, with $p^* \\in \\mathbb{R^3}$ the goal position, and $^0M_* \\in SE(3)$ the goal placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_cost_goal\n",
    "if REACH_DIMENSION == \"3d\":\n",
    "    # Cost for 3d tracking || p(q) - pref ||**2\n",
    "    goalTrackingRes = crocoddyl.ResidualModelFrameTranslation(state,FRAME_TIP,GOAL_POSITION)\n",
    "    goalTrackingCost = crocoddyl.CostModelResidual(state,goalTrackingRes)\n",
    "    runningCostModel.addCost(\"gripperPose\", goalTrackingCost, .001)\n",
    "    terminalCostModel.addCost(\"gripperPose\", goalTrackingCost, 1)\n",
    "elif REACH_DIMENSION == \"6d\":\n",
    "    # Cost for 6d tracking  || log( M(q)^-1 Mref ) ||**2\n",
    "    goal6TrackingRes = crocoddyl.ResidualModelFramePlacement(state,FRAME_TIP,GOAL_PLACEMENT)\n",
    "    goal6TrackingCost = crocoddyl.CostModelResidual(state,goal6TrackingRes)\n",
    "    runningCostModel.addCost(\"gripperPose\", goal6TrackingCost, .001)\n",
    "    terminalCostModel.addCost(\"gripperPose\", goal6TrackingCost, 1)\n",
    "else:\n",
    "    assert( REACH_DIMENSION==\"3d\" or REACH_DIMENSION==\"6d\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add regularization. First a state regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_cost_xreg\n",
    "# Cost for state regularization || x - x* ||**2\n",
    "# We set up different values for the integral cost and terminal cost term.\n",
    "\n",
    "# Regularization is stronger on position than velocity (to account for typical unit scale)\n",
    "xRegWeights = crocoddyl.ActivationModelWeightedQuad(np.array([1,1,1,1,1,1,1, .1,.1,.1,.1,.1,.1,.1]))\n",
    "xRegRes = crocoddyl.ResidualModelState(state,robot_model.x0)\n",
    "xRegCost = crocoddyl.CostModelResidual(state,xRegWeights,xRegRes)\n",
    "runningCostModel.addCost(\"xReg\", xRegCost, 1e-3)\n",
    "\n",
    "# Terminal cost for state regularization || x - x* ||**2\n",
    "# Require more strictly a small velocity at task end (but we don't car for the position)\n",
    "xRegWeightsT=crocoddyl.ActivationModelWeightedQuad(np.array([.5,.5,.5,.5,.5,.5,.5,  5.,5.,5.,5.,5.,5.,5.]))\n",
    "xRegResT = crocoddyl.ResidualModelState(state,robot_model.x0)\n",
    "xRegCostT = crocoddyl.CostModelResidual(state,xRegWeightsT,xRegResT)\n",
    "terminalCostModel.addCost(\"xReg\", xRegCostT, .01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then a control regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_cost_ureg\n",
    "# Cost for control regularization || u - g(q) ||**2\n",
    "uRegRes = crocoddyl.ResidualModelControlGrav(state)\n",
    "uRegCost = crocoddyl.CostModelResidual(state,uRegRes)\n",
    "runningCostModel.addCost(\"uReg\", uRegCost, 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action model \n",
    "Next, we need to create an action model for running and terminal nodes. We follow the same logic already explained with the bicopter: a differential action model (DAM) for the forward dynamics and the cost integrals, then a numerical integration in the integrator action model (IAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_iam\n",
    "# Next, we need to create the running and terminal action model.\n",
    "# The forward dynamics (computed using ABA) are implemented\n",
    "# inside DifferentialActionModelFullyActuated.\n",
    "\n",
    "# The actuation model is here trivial: tau_q = u.\n",
    "actuationModel = crocoddyl.ActuationModelFull(state)\n",
    "# Running model composing the costs, the differential equations of motion and the integrator.\n",
    "runningModel = crocoddyl.IntegratedActionModelEuler(\n",
    "    crocoddyl.DifferentialActionModelFreeFwdDynamics(state, actuationModel, runningCostModel), TIME_STEP)\n",
    "runningModel.differential.armature = robot_model.armature\n",
    "# Terminal model following the same logic, although the integration is here trivial.\n",
    "terminalModel = crocoddyl.IntegratedActionModelEuler(\n",
    "    crocoddyl.DifferentialActionModelFreeFwdDynamics(state, actuationModel, terminalCostModel), 0.)\n",
    "terminalModel.differential.armature = robot_model.armature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal control problem \n",
    "Once we have the action models, we just have to shape them into an optimal control problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_shoot\n",
    "problem = crocoddyl.ShootingProblem(robot_model.x0, [runningModel] * HORIZON_LENGTH, terminalModel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve and display\n",
    "We finalize the set up by creating the DDP solver for this optimal control problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_solve\n",
    "# Solving it using DDP\n",
    "# Create the DDP solver for this OC problem, verbose traces, with a logger\n",
    "ddp = crocoddyl.SolverDDP(problem)\n",
    "ddp.setCallbacks([\n",
    "    crocoddyl.CallbackLogger(),\n",
    "    crocoddyl.CallbackVerbose(),\n",
    "])\n",
    "\n",
    "# Solving it with the DDP algorithm\n",
    "ddp.solve([],[],1000)  # xs_init,us_init,maxiter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now look at the results, either by plotting it or animating the trajectory in the viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_plot\n",
    "# Plotting the solution and the DDP convergence\n",
    "log = ddp.getCallbacks()[0]\n",
    "crocoddyl.plotOCSolution(log.xs, log.us, figIndex=1, show=False)\n",
    "crocoddyl.plotConvergence(\n",
    "    log.costs,\n",
    "    log.pregs,\n",
    "    log.dregs,\n",
    "    log.grads,\n",
    "    log.stops,\n",
    "    log.steps,\n",
    "    figIndex=2,\n",
    "    show=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viewer.jupyter_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tp2/generated/panda_reaches_a_single_target_animate\n",
    "# # Visualizing the solution in gepetto-viewer\n",
    "for x in ddp.xs:\n",
    "    viz.display(x[:robot.model.nq])\n",
    "    time.sleep(TIME_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the example for reaching a sequence of targets\n",
    "\n",
    "Now we ask you to modify this example to reach a sequence of targets $p_{*1},p_{*2},p_{*3},p_{*4}$. The optimal trajectory should use similar regularization, but with the reaching cost now varying it time: for the first quarter of nodes, the target is $p_{*1}$, then another quarter with $p_{*2}$ etc until the four targets are reached. Don't specify a particular velocity when reaching the point to let more freedom to the solver.\n",
    "\n",
    "Below is a quick guideline to help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: prepare the environment\n",
    "\n",
    "Start by defining several targets (let's say 4 targets, all at x=0.4, and at y and z being either 0 or 0.4), and display then in the viewer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: define the shooting problem\n",
    "\n",
    "The shooting problem will be composed of 4 sequences of action models. Each sequence consists on T shooting \"running\" nodes and 1 terminal node. The running nodes mostly have regularization terms, while the terminal nodes have a strong cost toward the respective target.\n",
    "\n",
    "$[ R_1,R_1,R_1 ... R_1,T_1, R_2,R_2 .... R_2, T_2, R_3 ... R_3, T_3, R_4 ... R_4 ] , T4\n",
    "\n",
    "First create 4 running models and 4 terminal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to add a position cost, and state and control regularization to each running action model. Please  note that for terminal action model is only needed the position cost. Additionally, in the running models, the position cost should be low, and it should be high in the terminal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a shooting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq0 = [runningModels[0]]*T + [terminalModels[0]]\n",
    "seq1 = [runningModels[1]]*T + [terminalModels[1]]\n",
    "seq2 = [runningModels[2]]*T + [terminalModels[2]]\n",
    "seq3 = [runningModels[3]]*T \n",
    "problem = crocoddyl.ShootingProblem(x0,seq0+seq1+seq2+seq3,terminalmodel[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DDP solver for this problem and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp = crocoddyl.SolverDDP(problem)\n",
    "ddp.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it should not work, at least no on the first shot. The DDP solver is likely not strong enough to accept the random weights that you have selected. \n",
    "\n",
    "If it is working nicely from the first shot, display it in the viewer and go take a coffee. But you will likely have to tweak the gains to make it work.\n",
    "\n",
    "**It is suggested to first optimize only sequence 1. When you are happy with it, add sequence 2 and optimize again, etc.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening: toward hard constraints\n",
    "\n",
    "The solver works with double precisions, so it is quite robust to high weight. 10000 is likely to be accepted for example. But if you make the problem too difficult, the solver will break. \n",
    "In that case, you can implement a simple penalty solver by setting the weight to be 10**i, and creating a for loop to explore i from 0 to 5. At each iteration of the loop, run the solver from the previous solution and for few iterations only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(1,6):\n",
    "    for m in terminalModels:\n",
    "        m.costs.costs['gripperPose'].weight = 10**i\n",
    "    ddp.solve(ddp.xs, ddp.us, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This not very convenient, and a better solver should be used if you really want imposing hard constraints. Let's do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Adding hard constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last chapter, we guide you through the advanced, more recent, constraint interface. It is composed of two parts. First, constraints can be defined in the front end, following a similar logic to constraints, and using again the residual models. That should be pretty straight forward now you are proficient with cost models. Second, we need another solver. DDP is not aware of constraints and would just skip them. More advanced solvers are available. We propose here to use the recent SQP from the team \"Machine in Motion\" led by Ludovic Righetti at NYU, described in TODO ARXIV URL. TODO URL to MIM\n",
    "\n",
    "The implementation  is available in the mim-solver package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mim_solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the constraints\n",
    "\n",
    "Similarly to the sum-of-costs, all constraints must be stored in a constraint manager, which is given as initial argument when building a differential action model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose here to introduce a virtual wall constraining the end effector. For that, we use the same frame-translation residual. The constraint is defined along each axis with constant bounds, and np.inf when a particular direction should not be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can build a new action model with this constraint. We advise you don't constrain the initial node, as it easily leads to a unfeasible problem if $x_0$ does not satisfy the constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the SQP solver of MIM\n",
    "The solver follows a very similar syntax to the DDP solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it yourself.\n",
    "Now take the 4-point OCP you wrote in the first part. The motion finally obtained is typically a circle containing the four target points. \n",
    "### First exercice\n",
    "Add some bounds to sharpen the circle, so that the arcs are flatten by the virtual walls.\n",
    "### Second exercice\n",
    "Additionally add some constraints for the targets points, rather than defining them as costs. The constraints should then be equality, while they where inequalities up to now. This can be implemented by puting two equal bounds.\n",
    "### Third exercice\n",
    "Add joint limits. You will use the residual state for that. The bounds can be obtained from the URDF model through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_model.lowerPositionLimit, robot_model.upperPositionLimit, robot_model.velocityLimit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
